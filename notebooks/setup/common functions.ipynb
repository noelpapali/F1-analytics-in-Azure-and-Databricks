{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36abae2e-7974-4765-ac10-44961d0768aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "def add_ingestion_date(input_df):\n",
    "    output_df =input_df.withColumn(\"ingestion_date\", current_timestamp())\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3297d63-eef0-42cb-96cc-da8033ef96f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "def merge_delta_data(input_df, db_name, table_name, merge_condition, partitioncolumn, catalog=None):\n",
    "    spark.conf.set(\"spark.databricks.optimizer.dynamicPartitionPruning.enabled\", \"true\")\n",
    "\n",
    "    if catalog:\n",
    "        spark.sql(f\"USE CATALOG {catalog}\")\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{db_name}\")\n",
    "        ns = f\"{catalog}.{db_name}\"\n",
    "    else:\n",
    "        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name}\")\n",
    "        ns = db_name\n",
    "\n",
    "    full = f\"{ns}.{table_name}\"\n",
    "\n",
    "    # Existence check via SHOW TABLES (whitelisted)\n",
    "    exists = spark.sql(f\"SHOW TABLES IN {ns}\") \\\n",
    "                 .filter(f\"tableName = '{table_name}'\") \\\n",
    "                 .count() > 0\n",
    "\n",
    "    if exists:\n",
    "        (DeltaTable.forName(spark, full)\n",
    "            .alias(\"tgt\")\n",
    "            .merge(input_df.alias(\"src\"), merge_condition)\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute())\n",
    "    else:\n",
    "        (input_df.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .partitionBy(partitioncolumn)\n",
    "            .saveAsTable(full))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "common functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}